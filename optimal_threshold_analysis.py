# =============================================================================
# ë³´í–‰ ê°ì§€ ìµœì  ì„ê³„ê°’ ì¶”ì¶œ ì‹œìŠ¤í…œ
# ğŸ¯ ì´ì§„ ë¶„ë¥˜ ì ‘ê·¼ë²• + ROC ë¶„ì„ + êµì°¨ ê²€ì¦
# =============================================================================

import os
import glob
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re
from scipy.fft import fft, fftfreq
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import roc_curve, auc, classification_report, confusion_matrix
from sklearn.ensemble import RandomForestClassifier
import warnings
warnings.filterwarnings('ignore')

# êµ¬ê¸€ ë“œë¼ì´ë¸Œ ë§ˆìš´íŠ¸
from google.colab import drive
drive.mount('/content/drive')

# =============================================================================
# 1ë‹¨ê³„: ê°œì„ ëœ ì§ì ‘ ë°ì´í„° ë¡œë”©
# =============================================================================

def load_binary_classification_data(base_path='/content/drive/MyDrive/KFall_dataset/data/sensor_data_new'):
    """sensor_dataì—ì„œ ë°”ë¡œ ë³´í–‰/ë¹„ë³´í–‰ ë°ì´í„° ë¡œë”© (ê°œì„ ëœ ë°©ì‹)"""
    
    # í™œë™ ë¶„ë¥˜ ì •ì˜ (ë‚™ìƒ ê´€ë ¨ í™œë™ ì œì™¸)
    activity_mapping = {
        # ë³´í–‰ í™œë™ (Walking = 1)
        'walking': ['06', '07', '08', '09', '10', '35', '36'],
        
        # ë¹„ë³´í–‰ í™œë™ (Non-walking = 0) - ì¼ìƒ í™œë™ë§Œ í¬í•¨
        'non_walking': [
            '01', '02', '03', '04', '05',  # ì¼ìƒ í™œë™ (ì•‰ê¸°, ì„œê¸°, ëˆ„ì›Œìˆê¸° ë“±)
            '11', '12', '13', '14', '15',  # ê¸°íƒ€ í™œë™
            '16', '17', '18', '19'         # ê¸°íƒ€ ë¹„ë³´í–‰ í™œë™
            # 20~34 ë‚™ìƒ ê´€ë ¨ í™œë™ì€ ì œì™¸ (ë³„ë„ ë”¥ëŸ¬ë‹ ëª¨ë¸ì—ì„œ ì²˜ë¦¬)
        ]
    }
    
    all_data = []
    file_stats = {
        'walking_files': 0,
        'non_walking_files': 0,
        'excluded_fall_files': 0,
        'error_files': 0,
        'total_subjects': set()
    }
    
    print("ğŸ“‚ sensor_dataì—ì„œ ì§ì ‘ ë¡œë”© ì‹œì‘...")
    
    # ëª¨ë“  í”¼í—˜ì í´ë” íƒìƒ‰ (SA06~SA38, SA34 ì œì™¸)
    for subject_num in range(6, 39):
        if subject_num == 34:  # SA34 ì œì™¸
            continue
            
        subject_id = f'SA{subject_num:02d}'
        subject_folder = os.path.join(base_path, subject_id)
        
        if not os.path.exists(subject_folder):
            continue
            
        file_stats['total_subjects'].add(subject_id)
        
        # CSV íŒŒì¼ë“¤ íƒìƒ‰
        csv_files = glob.glob(os.path.join(subject_folder, '*.csv'))
        
        for csv_file in csv_files:
            filename = os.path.basename(csv_file)
            
            try:
                # íŒŒì¼ëª…ì—ì„œ Task ë²ˆí˜¸ ì¶”ì¶œ (ì˜ˆ: S06T06R01.csv â†’ 06)
                t_match = re.search(r'T(\d+)', filename)
                if not t_match:
                    continue
                    
                activity_num = t_match.group(1)
                
                # í™œë™ ë¶„ë¥˜
                if activity_num in activity_mapping['walking']:
                    label = 1  # ë³´í–‰
                    activity_type = 'walking'
                    file_stats['walking_files'] += 1
                elif activity_num in activity_mapping['non_walking']:
                    label = 0  # ë¹„ë³´í–‰
                    activity_type = 'non_walking'
                    file_stats['non_walking_files'] += 1
                elif int(activity_num) >= 20 and int(activity_num) <= 34:
                    # ë‚™ìƒ ê´€ë ¨ í™œë™ ì œì™¸
                    file_stats['excluded_fall_files'] += 1
                    continue
                else:
                    continue  # ì •ì˜ë˜ì§€ ì•Šì€ í™œë™
                
                # CSV íŒŒì¼ ë¡œë”© (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
                try:
                    df = pd.read_csv(csv_file)
                    
                    # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ (ë©”ëª¨ë¦¬ ì ˆì•½)
                    required_columns = ['TimeStamp(s)', 'AccX', 'AccY', 'AccZ', 'GyrX', 'GyrY', 'GyrZ']
                    missing_columns = [col for col in required_columns if col not in df.columns]
                    
                    if missing_columns:
                        print(f"âš ï¸ {filename}: ëˆ„ë½ëœ ì»¬ëŸ¼ {missing_columns}")
                        file_stats['error_files'] += 1
                        continue
                    
                    df = df[required_columns].copy()
                    
                    # ë©”íƒ€ë°ì´í„° ì¶”ê°€
                    df['subject_id'] = subject_id
                    df['activity_num'] = activity_num
                    df['label'] = label
                    df['activity_type'] = activity_type
                    df['filename'] = filename
                    
                    all_data.append(df)
                    
                except Exception as e:
                    print(f"âŒ {filename} ë¡œë”© ì‹¤íŒ¨: {e}")
                    file_stats['error_files'] += 1
                    continue
                    
            except Exception as e:
                print(f"âŒ {filename} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                file_stats['error_files'] += 1
                continue
    
    # ê²°ê³¼ í†µí•©
    if all_data:
        combined_df = pd.concat(all_data, ignore_index=True)
        
        print("\nâœ… ë¡œë”© ì™„ë£Œ!")
        print(f"ğŸ“Š ì´ í”¼í—˜ì: {len(file_stats['total_subjects'])}ëª…")
        print(f"ğŸš¶ ë³´í–‰ íŒŒì¼: {file_stats['walking_files']}ê°œ")
        print(f"ğŸ›‘ ë¹„ë³´í–‰ íŒŒì¼: {file_stats['non_walking_files']}ê°œ")
        print(f"ğŸš« ì œì™¸ëœ ë‚™ìƒ íŒŒì¼: {file_stats['excluded_fall_files']}ê°œ")
        print(f"âŒ ì˜¤ë¥˜ íŒŒì¼: {file_stats['error_files']}ê°œ")
        print(f"ğŸ“ˆ ì´ ë°ì´í„° í¬ì¸íŠ¸: {len(combined_df):,}ê°œ")
        print(f"âš–ï¸ ë³´í–‰ ë¹„ìœ¨: {combined_df['label'].mean():.2%}")
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶œë ¥
        memory_usage = combined_df.memory_usage(deep=True).sum() / 1024**2
        print(f"ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_usage:.1f} MB")
        
        return combined_df
    else:
        print("âŒ ë¡œë”©ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None

# =============================================================================
# 2ë‹¨ê³„: ìœˆë„ìš° ê¸°ë°˜ íŠ¹ì§• ì¶”ì¶œ
# =============================================================================

def extract_windowed_features(data, window_size=200, overlap=0.5):
    """ìœˆë„ìš° ê¸°ë°˜ íŠ¹ì§• ì¶”ì¶œ (ì‹¤ì œ ê°ì§€ê¸°ì™€ ë™ì¼í•œ ì¡°ê±´)"""
    
    def calculate_features(window_data):
        """ë‹¨ì¼ ìœˆë„ìš°ì—ì„œ íŠ¹ì§• ê³„ì‚°"""
        # ê°€ì†ë„ ë²¡í„° í¬ê¸°
        acc_mag = np.sqrt(window_data['AccX']**2 + window_data['AccY']**2 + window_data['AccZ']**2)
        gyr_mag = np.sqrt(window_data['GyrX']**2 + window_data['GyrY']**2 + window_data['GyrZ']**2)
        
        # ì´ë™í‰ê·  í•„í„°ë§
        acc_smooth = np.convolve(acc_mag, np.ones(5)/5, mode='same')
        
        # í”¼í¬ ê²€ì¶œ
        threshold = np.mean(acc_smooth) + 0.3 * np.std(acc_smooth)
        peaks = []
        for i in range(5, len(acc_smooth)-5):
            if acc_smooth[i] > threshold and acc_smooth[i] == max(acc_smooth[i-5:i+6]):
                peaks.append(i)
        
        # ê¸°ë³¸ í†µê³„ íŠ¹ì§•
        features = {
            'acc_mean': np.mean(acc_mag),
            'acc_std': np.std(acc_mag),
            'acc_max': np.max(acc_mag),
            'acc_min': np.min(acc_mag),
            'acc_range': np.max(acc_mag) - np.min(acc_mag),
            'gyr_mean': np.mean(gyr_mag),
            'gyr_std': np.std(gyr_mag),
            'peak_count': len(peaks),
            'peak_density': len(peaks) / len(acc_mag) * 100  # 100ìƒ˜í”Œë‹¹ í”¼í¬ ìˆ˜
        }
        
        # ë³´í–‰ ì£¼ê¸° íŠ¹ì§•
        if len(peaks) >= 2:
            time_stamps = window_data['TimeStamp(s)'].values
            peak_times = time_stamps[peaks]
            intervals = np.diff(peak_times)
            
            if len(intervals) > 0 and np.all(intervals > 0):  # ì–‘ìˆ˜ ê°„ê²©ë§Œ í—ˆìš©
                mean_interval = np.mean(intervals)
                std_interval = np.std(intervals)
                
                # ì•ˆì „í•œ ê³„ì‚° (0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€)
                if mean_interval > 0:
                    features['step_frequency'] = 1.0 / mean_interval
                else:
                    features['step_frequency'] = 0
                    
                features['step_regularity'] = 1.0 / (1.0 + std_interval) if std_interval >= 0 else 0
                features['step_interval_std'] = std_interval
            else:
                features['step_frequency'] = 0
                features['step_regularity'] = 0
                features['step_interval_std'] = 0
        else:
            features['step_frequency'] = 0
            features['step_regularity'] = 0
            features['step_interval_std'] = 0
        
        # ì£¼íŒŒìˆ˜ ë„ë©”ì¸ íŠ¹ì§• (ì•ˆì „í•œ ê³„ì‚°)
        try:
            if len(acc_mag) > 1:
                fft_acc = np.abs(fft(acc_mag))
                freqs = fftfreq(len(acc_mag), d=0.01)  # 100Hz ìƒ˜í”Œë§
                
                # 0.5-5Hz ëŒ€ì—­ì˜ ì—ë„ˆì§€ (ë³´í–‰ ì£¼íŒŒìˆ˜ ëŒ€ì—­)
                walking_band = (freqs >= 0.5) & (freqs <= 5.0)
                walking_energy = np.sum(fft_acc[walking_band])
                total_energy = np.sum(fft_acc)
                
                # ì•ˆì „í•œ ë¹„ìœ¨ ê³„ì‚°
                if total_energy > 0:
                    features['walking_band_energy'] = walking_energy
                    features['total_energy'] = total_energy
                    features['walking_energy_ratio'] = walking_energy / total_energy
                else:
                    features['walking_band_energy'] = 0
                    features['total_energy'] = 0
                    features['walking_energy_ratio'] = 0
            else:
                features['walking_band_energy'] = 0
                features['total_energy'] = 0
                features['walking_energy_ratio'] = 0
        except:
            # FFT ê³„ì‚° ì‹¤íŒ¨ì‹œ ê¸°ë³¸ê°’
            features['walking_band_energy'] = 0
            features['total_energy'] = 0
            features['walking_energy_ratio'] = 0
        
        # ë¬´í•œëŒ€ ê°’ê³¼ NaN ê°’ ì²˜ë¦¬
        for key, value in features.items():
            if np.isnan(value) or np.isinf(value):
                features[key] = 0.0
            elif value > 1e10:  # ë„ˆë¬´ í° ê°’ ì œí•œ
                features[key] = 1e10
            elif value < -1e10:  # ë„ˆë¬´ ì‘ì€ ê°’ ì œí•œ
                features[key] = -1e10
        
        return features
    
    # ìœˆë„ìš°ë³„ íŠ¹ì§• ì¶”ì¶œ
    all_features = []
    step_size = int(window_size * (1 - overlap))
    
    grouped = data.groupby(['subject_id', 'activity_num', 'label', 'activity_type'])
    
    for (subject, activity, label, act_type), group in grouped:
        # ì‹œê°„ìˆœ ì •ë ¬
        group = group.sort_values('TimeStamp(s)')
        
        # ìœˆë„ìš° ìŠ¬ë¼ì´ë”©
        for start_idx in range(0, len(group) - window_size + 1, step_size):
            window = group.iloc[start_idx:start_idx + window_size]
            
            if len(window) == window_size:  # ì™„ì „í•œ ìœˆë„ìš°ë§Œ ì‚¬ìš©
                features = calculate_features(window)
                features.update({
                    'subject_id': subject,
                    'activity_num': activity,
                    'label': label,
                    'activity_type': act_type
                })
                all_features.append(features)
    
    features_df = pd.DataFrame(all_features)
    print(f"âœ… {len(features_df)}ê°œ ìœˆë„ìš° íŠ¹ì§• ì¶”ì¶œ ì™„ë£Œ")
    print(f"ğŸš¶ ë³´í–‰ ìœˆë„ìš°: {len(features_df[features_df['label']==1]):,}ê°œ")
    print(f"ğŸ›‘ ë¹„ë³´í–‰ ìœˆë„ìš°: {len(features_df[features_df['label']==0]):,}ê°œ")
    
    return features_df

# =============================================================================
# 3ë‹¨ê³„: ROC ê¸°ë°˜ ìµœì  ì„ê³„ê°’ ê³„ì‚°
# =============================================================================

def find_optimal_thresholds_roc(features_df):
    """ROC ë¶„ì„ì„ í†µí•œ ìµœì  ì„ê³„ê°’ ê³„ì‚°"""
    
    # íŠ¹ì§• ì„ íƒ
    feature_columns = [
        'acc_mean', 'acc_std', 'acc_range', 'gyr_mean', 'gyr_std',
        'step_frequency', 'step_regularity', 'peak_count', 'peak_density',
        'walking_energy_ratio'
    ]
    
    # ë°ì´í„° ì •ë¦¬ ë° ê²€ì¦
    print("ğŸ” ë°ì´í„° ê²€ì¦ ë° ì •ë¦¬ ì¤‘...")
    
    # ë¬´í•œëŒ€ ê°’ê³¼ NaN ê°’ í™•ì¸
    for col in feature_columns:
        if col in features_df.columns:
            # ë¬´í•œëŒ€ ê°’ ì²˜ë¦¬
            inf_mask = np.isinf(features_df[col])
            if inf_mask.any():
                print(f"âš ï¸ {col}: {inf_mask.sum()}ê°œ ë¬´í•œëŒ€ ê°’ ë°œê²¬ â†’ 0ìœ¼ë¡œ ëŒ€ì²´")
                features_df.loc[inf_mask, col] = 0.0
            
            # NaN ê°’ ì²˜ë¦¬
            nan_mask = np.isnan(features_df[col])
            if nan_mask.any():
                print(f"âš ï¸ {col}: {nan_mask.sum()}ê°œ NaN ê°’ ë°œê²¬ â†’ 0ìœ¼ë¡œ ëŒ€ì²´")
                features_df.loc[nan_mask, col] = 0.0
            
            # ë„ˆë¬´ í° ê°’ ì œí•œ
            large_mask = np.abs(features_df[col]) > 1e10
            if large_mask.any():
                print(f"âš ï¸ {col}: {large_mask.sum()}ê°œ í° ê°’ ë°œê²¬ â†’ ì œí•œ")
                features_df.loc[large_mask, col] = np.sign(features_df.loc[large_mask, col]) * 1e10
    
    X = features_df[feature_columns]
    y = features_df['label']
    
    # ìµœì¢… ë°ì´í„° ê²€ì¦
    print(f"ğŸ“Š ìµœì¢… ë°ì´í„° ê²€ì¦:")
    print(f"   ë¬´í•œëŒ€ ê°’: {np.isinf(X).sum().sum()}ê°œ")
    print(f"   NaN ê°’: {np.isnan(X).sum().sum()}ê°œ")
    print(f"   ìœ íš¨í•œ ìƒ˜í”Œ: {len(X)}ê°œ")
    
    # í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í•  (í”¼í—˜ìë³„ ë¶„í• )
    subjects = features_df['subject_id'].unique()
    train_subjects, test_subjects = train_test_split(subjects, test_size=0.3, random_state=42)
    
    train_mask = features_df['subject_id'].isin(train_subjects)
    test_mask = features_df['subject_id'].isin(test_subjects)
    
    X_train, X_test = X[train_mask], X[test_mask]
    y_train, y_test = y[train_mask], y[test_mask]
    
    print(f"ğŸ“Š í›ˆë ¨ ë°ì´í„°: {len(X_train)}ê°œ ({y_train.mean():.2%} ë³´í–‰)")
    print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(X_test)}ê°œ ({y_test.mean():.2%} ë³´í–‰)")
    
    # ê° íŠ¹ì§•ë³„ ìµœì  ì„ê³„ê°’ ê³„ì‚°
    optimal_thresholds = {}
    
    plt.figure(figsize=(15, 10))
    
    for i, feature in enumerate(feature_columns):
        plt.subplot(2, 5, i+1)
        
        try:
            # íŠ¹ì§• ê°’ ë²”ìœ„ í™•ì¸
            feature_values = X_train[feature]
            print(f"ğŸ” {feature}: ë²”ìœ„ [{feature_values.min():.3f}, {feature_values.max():.3f}]")
            
            # ROC ê³¡ì„  ê³„ì‚°
            fpr, tpr, thresholds = roc_curve(y_train, feature_values)
            roc_auc = auc(fpr, tpr)
            
            # Youden's J statisticìœ¼ë¡œ ìµœì ì  ì°¾ê¸°
            j_scores = tpr - fpr
            optimal_idx = np.argmax(j_scores)
            optimal_threshold = thresholds[optimal_idx]
            
            optimal_thresholds[feature] = {
                'threshold': optimal_threshold,
                'auc': roc_auc,
                'sensitivity': tpr[optimal_idx],
                'specificity': 1 - fpr[optimal_idx]
            }
            
            # ROC ê³¡ì„  ê·¸ë¦¬ê¸°
            plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.3f}')
            plt.plot(fpr[optimal_idx], tpr[optimal_idx], 'ro', markersize=8)
            plt.plot([0, 1], [0, 1], 'k--')
            plt.xlabel('False Positive Rate')
            plt.ylabel('True Positive Rate')
            plt.title(f'{feature}\nThreshold: {optimal_threshold:.3f}')
            plt.legend()
            plt.grid(True, alpha=0.3)
            
        except Exception as e:
            print(f"âŒ {feature} ROC ê³„ì‚° ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ê°’ ì„¤ì •
            optimal_thresholds[feature] = {
                'threshold': 0.5,
                'auc': 0.5,
                'sensitivity': 0.5,
                'specificity': 0.5
            }
            plt.text(0.5, 0.5, f'Error: {feature}', ha='center', va='center')
            plt.title(f'{feature}\nError')
    
    plt.tight_layout()
    plt.show()
    
    return optimal_thresholds, (X_train, X_test, y_train, y_test)

# =============================================================================
# 4ë‹¨ê³„: ì•™ìƒë¸” ê¸°ë°˜ ì„ê³„ê°’ ìµœì í™”
# =============================================================================

def optimize_ensemble_thresholds(features_df, optimal_thresholds):
    """ì•™ìƒë¸” ë°©ì‹ìœ¼ë¡œ ìµœì¢… ì„ê³„ê°’ ìµœì í™”"""
    
    # í•µì‹¬ íŠ¹ì§•ë“¤ ì„ íƒ (AUC ê¸°ì¤€)
    feature_importance = {k: v['auc'] for k, v in optimal_thresholds.items()}
    top_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)[:6]
    
    print("ğŸ† ìƒìœ„ íŠ¹ì§•ë“¤ (AUC ê¸°ì¤€):")
    for feature, auc_score in top_features:
        print(f"   {feature}: {auc_score:.3f}")
    
    # ë³´í–‰ ê°ì§€ê¸°ì™€ ìœ ì‚¬í•œ ê·œì¹™ ê¸°ë°˜ ì‹œìŠ¤í…œ êµ¬ì„±
    walking_rules = {
        'acc_mean_range': (
            optimal_thresholds['acc_mean']['threshold'] * 0.9,  # í•˜í•œ
            optimal_thresholds['acc_mean']['threshold'] * 1.1   # ìƒí•œ
        ),
        'acc_std_min': optimal_thresholds['acc_std']['threshold'],
        'step_freq_range': (1.0, 4.0),  # ìƒë¦¬í•™ì  ë²”ìœ„
        'step_regularity_min': optimal_thresholds['step_regularity']['threshold'],
        'walking_energy_ratio_min': optimal_thresholds['walking_energy_ratio']['threshold']
    }
    
    # ê°€ì¤‘ì¹˜ ìµœì í™” (ê·¸ë¦¬ë“œ ì„œì¹˜)
    best_weights = None
    best_f1 = 0
    
    weight_combinations = [
        {'acc': 0.25, 'std': 0.25, 'freq': 0.35, 'reg': 0.15},
        {'acc': 0.20, 'std': 0.30, 'freq': 0.40, 'reg': 0.10},
        {'acc': 0.30, 'std': 0.20, 'freq': 0.30, 'reg': 0.20},
        {'acc': 0.15, 'std': 0.25, 'freq': 0.45, 'reg': 0.15},
    ]
    
    for weights in weight_combinations:
        # ê·œì¹™ ê¸°ë°˜ ë¶„ë¥˜ ìˆ˜í–‰
        predictions = []
        for _, row in features_df.iterrows():
            confidence = 0.0
            
            # ê°€ì†ë„ í‰ê·  ì²´í¬
            if walking_rules['acc_mean_range'][0] <= row['acc_mean'] <= walking_rules['acc_mean_range'][1]:
                confidence += weights['acc']
            
            # ê°€ì†ë„ í‘œì¤€í¸ì°¨ ì²´í¬
            if row['acc_std'] >= walking_rules['acc_std_min']:
                confidence += weights['std']
            
            # ë³´í–‰ ì£¼íŒŒìˆ˜ ì²´í¬
            if walking_rules['step_freq_range'][0] <= row['step_frequency'] <= walking_rules['step_freq_range'][1]:
                confidence += weights['freq']
            
            # ê·œì¹™ì„± ì²´í¬
            if row['step_regularity'] >= walking_rules['step_regularity_min']:
                confidence += weights['reg']
            
            predictions.append(1 if confidence >= 0.6 else 0)
        
        # F1 ìŠ¤ì½”ì–´ ê³„ì‚°
        from sklearn.metrics import f1_score
        f1 = f1_score(features_df['label'], predictions)
        
        if f1 > best_f1:
            best_f1 = f1
            best_weights = weights
    
    print(f"\nğŸ¯ ìµœì  ê°€ì¤‘ì¹˜ (F1: {best_f1:.3f}):")
    for key, value in best_weights.items():
        print(f"   {key}: {value}")
    
    return walking_rules, best_weights

# =============================================================================
# 5ë‹¨ê³„: ìµœì¢… ì„ê³„ê°’ ìƒì„±
# =============================================================================

def generate_final_thresholds(optimal_thresholds, walking_rules, best_weights):
    """ìµœì¢… ì„ê³„ê°’ ë”•ì…”ë„ˆë¦¬ ìƒì„±"""
    
    final_config = {
        'thresholds': {
            # ê°€ì†ë„ ê´€ë ¨
            'acc_mean_min': walking_rules['acc_mean_range'][0],
            'acc_mean_max': walking_rules['acc_mean_range'][1],
            'acc_std_min': walking_rules['acc_std_min'],
            
            # ë³´í–‰ ì£¼ê¸° ê´€ë ¨
            'step_freq_min': walking_rules['step_freq_range'][0],
            'step_freq_max': walking_rules['step_freq_range'][1],
            'regularity_min': walking_rules['step_regularity_min'],
            
            # í”¼í¬ ê²€ì¶œ ê´€ë ¨
            'peak_detection_factor': 0.3,
            'peak_window_size': 5,
            
            # ìµœì¢… íŒë‹¨
            'confidence_min': 0.6
        },
        'weights': {
            'acc_mean_weight': best_weights['acc'],
            'acc_std_weight': best_weights['std'],
            'step_freq_weight': best_weights['freq'],
            'regularity_weight': best_weights['reg']
        },
        'filtering': {
            'moving_avg_window': 5,
            'min_peaks_required': 2
        }
    }
    
    print("\nğŸ¯ ìµœì¢… ìµœì í™”ëœ ì„¤ì •:")
    print("ğŸ“Š ì„ê³„ê°’:")
    for key, value in final_config['thresholds'].items():
        print(f"   {key}: {value:.3f}")
    print("âš–ï¸ ê°€ì¤‘ì¹˜:")
    for key, value in final_config['weights'].items():
        print(f"   {key}: {value:.3f}")
    
    return final_config

# =============================================================================
# ì „ì²´ ì‹¤í–‰ í•¨ìˆ˜
# =============================================================================

def run_optimal_threshold_analysis():
    """ìµœì  ì„ê³„ê°’ ë¶„ì„ ì „ì²´ ì‹¤í–‰"""
    
    print("ğŸš€ ë³´í–‰ ê°ì§€ ìµœì  ì„ê³„ê°’ ë¶„ì„ ì‹œì‘")
    print("="*60)
    
    # 1ë‹¨ê³„: ì´ì§„ ë¶„ë¥˜ ë°ì´í„° ë¡œë”©
    print("\n1ï¸âƒ£ ì´ì§„ ë¶„ë¥˜ ë°ì´í„° ë¡œë”©...")
    data = load_binary_classification_data()
    if data is None:
        return
    
    # 2ë‹¨ê³„: ìœˆë„ìš° ê¸°ë°˜ íŠ¹ì§• ì¶”ì¶œ
    print("\n2ï¸âƒ£ ìœˆë„ìš° ê¸°ë°˜ íŠ¹ì§• ì¶”ì¶œ...")
    features_df = extract_windowed_features(data, window_size=200, overlap=0.5)
    
    # 3ë‹¨ê³„: ROC ê¸°ë°˜ ìµœì  ì„ê³„ê°’ ê³„ì‚°
    print("\n3ï¸âƒ£ ROC ë¶„ì„...")
    optimal_thresholds, split_data = find_optimal_thresholds_roc(features_df)
    
    # 4ë‹¨ê³„: ì•™ìƒë¸” ìµœì í™”
    print("\n4ï¸âƒ£ ì•™ìƒë¸” ìµœì í™”...")
    walking_rules, best_weights = optimize_ensemble_thresholds(features_df, optimal_thresholds)
    
    # 5ë‹¨ê³„: ìµœì¢… ì„¤ì • ìƒì„±
    print("\n5ï¸âƒ£ ìµœì¢… ì„¤ì • ìƒì„±...")
    final_config = generate_final_thresholds(optimal_thresholds, walking_rules, best_weights)
    
    print("\nâœ… ìµœì  ì„ê³„ê°’ ë¶„ì„ ì™„ë£Œ!")
    print("ğŸ¯ ì´ ì„¤ì •ì„ walking_detector_raspberry.pyì— ì ìš©í•˜ì„¸ìš”.")
    
    return final_config, features_df, optimal_thresholds

# =============================================================================
# ì‹¤í–‰
# =============================================================================

if __name__ == "__main__":
    results = run_optimal_threshold_analysis() 